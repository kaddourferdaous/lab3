{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f30df9fd-f4c6-4586-b4a5-d93087a51160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DeLL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.aljazeera.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\DeLL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.alriyadh.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as arabic_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URLs\n",
    "urls = [\n",
    "    \"https://www.aljazeera.net\",  # Exemple d'un site d'actualités en arabe\n",
    "    \"https://www.alriyadh.com\",  # Exemple d'un site saoudien\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "# Loop through each URL\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Temporarily bypass SSL verification\n",
    "        response = requests.get(url, verify=False)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract paragraphs (you can modify this based on your needs)\n",
    "            articles = soup.find_all('p')  # Example: Extract paragraphs\n",
    "            for article in articles:\n",
    "                text = article.get_text(strip=True)\n",
    "                if text:  # Ensure the text is not empty\n",
    "                    data.append(text)\n",
    "        else:\n",
    "            print(f\"Failed to fetch {url}: HTTP {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "\n",
    "# Save the data to a DataFrame\n",
    "df = pd.DataFrame(data, columns=['Text'])\n",
    "\n",
    "# Export the dataset to a CSV file\n",
    "df.to_csv(\"arabic_dataset.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"Dataset saved as arabic_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd08820e-a78c-4997-9ccb-091c8b2c7937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text\n",
      "0  لم يكن علماء السلف على مذهب فقهي واحد في معامل...\n",
      "1  تناولت حلقة (2024/12/3) من برنامج ” الاتجاه ال...\n",
      "2  أفاد تحقيق لهآرتس، بأن موجة معاداة السامية الت...\n",
      "3  اتهم مدير مستشفى كمال عدوان بشمال غزة الجيش ال...\n",
      "4  قالت لوموند إن الحكومة السورية تدفع ثمن رفضها ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger les données depuis le fichier CSV\n",
    "df = pd.read_csv(\"arabic_dataset.csv\")\n",
    "\n",
    "# Afficher les premières lignes pour vérifier les données\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5c71b1c-397b-426e-abef-4e426b70a3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DeLL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DeLL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DeLL\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement réussi !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Téléchargez les ressources nécessaires\n",
    "nltk.download('punkt')        # Cela téléchargera les modèles de tokenisation de base\n",
    "nltk.download('stopwords')    # Téléchargez les stopwords\n",
    "nltk.download('punkt_tab')    # Téléchargez le modèle de tokenisation pour d'autres langues\n",
    "\n",
    "# Vérifiez si le téléchargement est effectué correctement\n",
    "print(\"Téléchargement réussi !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e283ad60-eca9-453a-b0ae-4a65a9f7e6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens sans stopwords: ['الذكاء', 'الاصطناعي', 'مجال', 'علوم', 'الكمبيوتر', 'يهتم', 'بتطوير', 'الأنظمة', 'الذكية.']\n",
      "Tokens nettoyés: ['الذكاء', 'الاصطناعي', 'مجال', 'علوم', 'الكمبيوتر', 'يهتم', 'بتطوير', 'الأنظمة', 'الذكية.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DeLL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Exemple de texte arabe\n",
    "text = \"الذكاء الاصطناعي هو مجال في علوم الكمبيوتر يهتم بتطوير الأنظمة الذكية.\"\n",
    "\n",
    "# Tokenisation manuelle\n",
    "tokens = text.split()\n",
    "\n",
    "# Charger la liste des stopwords en arabe\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('arabic'))\n",
    "\n",
    "# Filtrer les tokens en supprimant les stopwords\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(\"Tokens sans stopwords:\", filtered_tokens)\n",
    "\n",
    "# Fonction pour supprimer la ponctuation\n",
    "def remove_punctuation(tokens):\n",
    "    return [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "# Supprimer la ponctuation des tokens\n",
    "cleaned_tokens = remove_punctuation(filtered_tokens)\n",
    "\n",
    "print(\"Tokens nettoyés:\", cleaned_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ac438d6-a3f5-44fd-9039-6fc48d5959c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens après stemming: ['الذكاء', 'الاصطناعي', 'مجال', 'علوم', 'الكمبيوتر', 'يهتم', 'بتطوير', 'الأنظمة', 'الذكية.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialiser le stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Appliquer le stemming\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"Tokens après stemming:\", stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21c5b3f6-887d-4289-a8be-fe8c236975df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens après lemmatisation: ['الذكاء', 'الاصطناعي', 'مجال', 'علوم', 'الكمبيوتر', 'يهتم', 'بتطوير', 'الأنظمة', 'الذكية.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialiser le lemmatiseur\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Appliquer la lemmatisation\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"Tokens après lemmatisation:\", lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63aa1bfc-e390-4ce7-8449-b19dc06afbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données discrétisées: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# Exemple de données numériques continues\n",
    "data = [[1], [2], [3], [4], [5]]\n",
    "\n",
    "# Initialiser le discrétiseur\n",
    "discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "\n",
    "# Appliquer la discrétisation\n",
    "discretized_data = discretizer.fit_transform(data)\n",
    "print(\"Données discrétisées:\", discretized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9e60d89-af6c-41c5-bdee-2692f5ba1c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "425487fd-194b-49ea-ac68-bc63750f6e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 9\n",
      "Max Sequence Length: 6\n",
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.6366 - val_accuracy: 1.0000 - val_loss: 0.5538\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 1.0000 - loss: 0.5538 - val_accuracy: 1.0000 - val_loss: 0.4777\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 1.0000 - loss: 0.4777 - val_accuracy: 1.0000 - val_loss: 0.4073\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 1.0000 - loss: 0.4073 - val_accuracy: 1.0000 - val_loss: 0.3423\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 1.0000 - loss: 0.3423 - val_accuracy: 1.0000 - val_loss: 0.2828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f2bc6e7190>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding\n",
    "import numpy as np\n",
    "\n",
    "# Exemple de données textuelles\n",
    "texts = [\"Exemple de texte pour la tokenisation\", \"Un autre exemple de texte\"]\n",
    "\n",
    "# 1. Créer un tokenizer pour transformer les mots en indices\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# 2. Définir vocab_size : le nombre total de mots uniques\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 pour inclure le token de padding\n",
    "\n",
    "# 3. Convertir les textes en séquences d'indices\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# 4. Définir max_sequence_length : la longueur maximale des séquences (par exemple, 10)\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "# 5. Appliquer du padding pour avoir des séquences de même longueur\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# 6. Définir les étiquettes (par exemple, classification binaire)\n",
    "y_train = np.array([1, 0])  # Exemple d'étiquettes binaires pour deux exemples\n",
    "y_val = np.array([1, 0])  # Exemple d'étiquettes binaires pour la validation\n",
    "\n",
    "# Afficher vocab_size et max_sequence_length\n",
    "print(\"Vocab Size:\", vocab_size)\n",
    "print(\"Max Sequence Length:\", max_sequence_length)\n",
    "\n",
    "# Créer et entraîner le modèle RNN\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_sequence_length))\n",
    "model_rnn.add(SimpleRNN(64, return_sequences=False))\n",
    "model_rnn.add(Dense(1, activation='sigmoid'))  # Si c'est un problème de classification binaire\n",
    "\n",
    "# Compiler et entraîner le modèle\n",
    "model_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_rnn.fit(padded_sequences, y_train, epochs=5, batch_size=32, validation_data=(padded_sequences, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c86e098-184d-4e47-a0e8-34b714ca3836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Bidirectional RNN model:\n",
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 0.6695 - val_accuracy: 1.0000 - val_loss: 0.5597\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 1.0000 - loss: 0.5597 - val_accuracy: 1.0000 - val_loss: 0.4634\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 0.4634 - val_accuracy: 1.0000 - val_loss: 0.3785\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 1.0000 - loss: 0.3785 - val_accuracy: 1.0000 - val_loss: 0.3041\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 0.3041 - val_accuracy: 1.0000 - val_loss: 0.2396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f2bc60f5d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, SimpleRNN, Dense, Embedding\n",
    "\n",
    "# ===============================\n",
    "# 2. MODELE BIDIRECTIONNEL RNN\n",
    "# ===============================\n",
    "model_bidir_rnn = Sequential()\n",
    "model_bidir_rnn.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_sequence_length))\n",
    "model_bidir_rnn.add(Bidirectional(SimpleRNN(64, return_sequences=False)))\n",
    "model_bidir_rnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_bidir_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"\\nTraining Bidirectional RNN model:\")\n",
    "model_bidir_rnn.fit(padded_sequences, y_train, epochs=5, batch_size=32, validation_data=(padded_sequences, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8872bdb2-4bfe-47eb-a14b-6d63bad11df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training GRU model:\n",
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.5000 - loss: 0.6989 - val_accuracy: 1.0000 - val_loss: 0.6860\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 1.0000 - loss: 0.6860 - val_accuracy: 1.0000 - val_loss: 0.6733\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 1.0000 - loss: 0.6733 - val_accuracy: 1.0000 - val_loss: 0.6607\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 1.0000 - loss: 0.6607 - val_accuracy: 1.0000 - val_loss: 0.6479\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 1.0000 - loss: 0.6479 - val_accuracy: 1.0000 - val_loss: 0.6349\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f2c00efb10>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Embedding\n",
    "\n",
    "# ===============================\n",
    "# 3. MODELE GRU\n",
    "# ===============================\n",
    "model_gru = Sequential()\n",
    "model_gru.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_sequence_length))\n",
    "model_gru.add(GRU(64, return_sequences=False))\n",
    "model_gru.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_gru.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"\\nTraining GRU model:\")\n",
    "model_gru.fit(padded_sequences, y_train, epochs=5, batch_size=32, validation_data=(padded_sequences, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "048808c3-0e67-423b-93bf-c8bc687613bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM model:\n",
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.0000e+00 - loss: 0.6958 - val_accuracy: 1.0000 - val_loss: 0.6898\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 0.6898 - val_accuracy: 1.0000 - val_loss: 0.6837\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 1.0000 - loss: 0.6837 - val_accuracy: 1.0000 - val_loss: 0.6775\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 0.6775 - val_accuracy: 1.0000 - val_loss: 0.6711\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 0.6711 - val_accuracy: 1.0000 - val_loss: 0.6645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f2c519e8d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "\n",
    "# ===============================\n",
    "# 4. MODELE LSTM\n",
    "# ===============================\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_sequence_length))\n",
    "model_lstm.add(LSTM(64, return_sequences=False))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"\\nTraining LSTM model:\")\n",
    "model_lstm.fit(padded_sequences, y_train, epochs=5, batch_size=32, validation_data=(padded_sequences, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70c7af7f-84b7-4329-9a0f-6385e4962d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c740757-a1b3-4528-8cd4-909857e31f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
